{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalGaussian(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_dim: int, hidden_dim: int, action_dim: int, activation\n",
    "    ) -> None:\n",
    "        super(DiagonalGaussian, self).__init__()\n",
    "        log_std = -0.5 * np.ones(action_dim, dtype=float)\n",
    "        self.covariance_matrix = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "        self.mean_action_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            activation,\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def _distribution(self, observation):\n",
    "        mean_act = self.mean_action_net(observation)\n",
    "        covariance_mat = torch.exp(self.covariance_matrix)\n",
    "        return Normal(mean_act, covariance_mat)\n",
    "\n",
    "    def _log_probs_from_dist(self, policy_dist, action):\n",
    "        return policy_dist.log_prob(action).sum(axis=-1)\n",
    "\n",
    "    def forward(self, observation, action=None):\n",
    "        policy_dist = self._distribution(observation)\n",
    "        logp_act = None\n",
    "        if action is not None:\n",
    "            logp_act = self._log_probs_from_dist(policy_dist, action)\n",
    "        return policy_dist, logp_act\n",
    "\n",
    "\n",
    "class ValueFunctionLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_dim: int, hidden_dim: int, action_dim: int, activation\n",
    "    ) -> None:\n",
    "        super(ValueFunctionLearner, self).__init__()\n",
    "        self.v_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            activation,\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, observation):\n",
    "        # return torch.squeeze(self.v_net(observation), -1)\n",
    "        return self.v_net(observation)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 32,\n",
    "        activation=nn.Softmax(dim=-1),\n",
    "    ) -> None:\n",
    "        super(Agent, self).__init__()\n",
    "        self.policy = DiagonalGaussian(obs_dim, hidden_dim, action_dim, activation)\n",
    "        self.value_func = ValueFunctionLearner(\n",
    "            obs_dim, hidden_dim, action_dim, activation\n",
    "        )\n",
    "\n",
    "    def step(self, obs: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            policy_dist = self.policy._distribution(obs)\n",
    "            action = policy_dist.sample()\n",
    "            action[0] = 1\n",
    "            mean_action = self.policy._log_probs_from_dist(policy_dist, action)\n",
    "            value = self.value_func(obs)\n",
    "        return action.numpy(), value.numpy(), mean_action.numpy()\n",
    "\n",
    "    def act(self, obs: torch.Tensor):\n",
    "        return self.step(obs)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Advantage(ABC):\n",
    "    @abstractmethod\n",
    "    def estimate(self, values: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "class ReturnEstimator(ABC):\n",
    "    @abstractmethod\n",
    "    def get_return(self, rewards: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "@dataclass\n",
    "class DiscountReturn(ReturnEstimator):\n",
    "    gamma: Optional[float] = 0.99\n",
    "\n",
    "    def get_return(self, rewards: np.ndarray) -> np.ndarray:\n",
    "        pot = np.cumsum(np.ones(len(rewards))) - 1\n",
    "        g = np.full(len(pot), fill_value=self.gamma)\n",
    "        discount_gamma = g**pot\n",
    "        return rewards * discount_gamma\n",
    "        \n",
    "@dataclass\n",
    "class GAE(Advantage):\n",
    "    rewards: np.ndarray\n",
    "    values: np.ndarray\n",
    "    return_estimator: ReturnEstimator\n",
    "    lamda: Optional[float] = 0.5\n",
    "    gamma: Optional[float] = 0.99\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.batch_size: int = self.rewards.shape[0]\n",
    "        self.adv: np.ndarray = np.zeros(self.batch_size)\n",
    "        self.returns: np.ndarray() = np.zeros(self.batch_size)\n",
    "        self.return_estimator = self.return_estimator(gamma=self.lamda*self.gamma)\n",
    "\n",
    "    def estimate(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        rew = np.append(self.rewards, 0)\n",
    "        val = np.append(self.values, 0)\n",
    "        deltas = rew[:-1] + (self.gamma * val[1:]) - val[:-1]\n",
    "        self.adv = self.discount_return.get_return(deltas)\n",
    "        self.discount_return.gamma = self.gamma\n",
    "        self.returns = self.discount_return.get_return(rew)[:-1] # value function targets\n",
    "        return self.adv, self.returns\n",
    "\n",
    "\n",
    "class TrajectoryReplayBuffer:\n",
    "    \"\"\"A buffer class for storing trajectory data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        advantage: Advantage,\n",
    "        obs_dim: int,\n",
    "        act_dim: int,\n",
    "        val_dim: int,\n",
    "        buf_size: int = 20,\n",
    "    ) -> None:\n",
    "        self._buf_size = buf_size\n",
    "        self._adv_estimator = advantage\n",
    "        self._obs = np.zeros((buf_size, obs_dim), dtype=float)\n",
    "        self._act = np.zeros((buf_size, act_dim), dtype=float)\n",
    "        self._val = np.zeros((buf_size, val_dim), dtype=float)\n",
    "        self._adv = np.zeros(buf_size, dtype=float)\n",
    "        self._mean_act = np.zeros(buf_size, dtype=float)\n",
    "        self._rewards = np.zeros(buf_size, dtype=float)\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        idx: int,\n",
    "        action: np.ndarray,\n",
    "        value: np.ndarray,\n",
    "        reward: float,\n",
    "        mean_act: float,\n",
    "    ) -> None:\n",
    "        assert idx < self._buf_size\n",
    "        self._act[idx] = action\n",
    "        self._val[idx] = value\n",
    "        self._rewards[idx] = reward\n",
    "        self._mean_act[idx] = mean_act\n",
    "\n",
    "    def compute_advantage(self):\n",
    "        adv_ = self._adv_estimator(self._rewards, self._val)\n",
    "        self._adv, val_func_targets = adv_.estimate()\n",
    "        return self._adv, val_func_targets\n",
    "\n",
    "    def expected_returns(self, arr: np.ndarray) -> np.ndarray:\n",
    "        expected_returns = np.zeros(arr.shape)\n",
    "        for i in reversed(range(len(arr))):\n",
    "            ret_t = self._return_estimator(arr[i:])\n",
    "            expected_returns[i] = ret_t\n",
    "        return expected_returns\n",
    "\n",
    "    def get_trajectories(self):\n",
    "        data = dict(V=self._val)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in data.items()}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
