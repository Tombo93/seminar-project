{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ContinousControl_ParallelTestEnv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependancies"
      ],
      "metadata": {
        "id": "5sCdH_XokCJy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8s2bFrpvI2Wl"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Model\n",
        "* Diagonal Gaussian\n",
        "  - 2 MLPs\n",
        "* Value function Learner\n",
        "  - 1 MLP\n",
        "* Actor\n",
        "  - Class to tie both together"
      ],
      "metadata": {
        "id": "C0bDNpgckIcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiagonalGaussian(nn.Module):\n",
        "    def __init__(\n",
        "        self, obs_dim: int, hidden_dim: int, action_dim: int, activation\n",
        "    ) -> None:\n",
        "        super(DiagonalGaussian, self).__init__()\n",
        "        log_std = -0.5 * np.ones(action_dim, dtype=float)\n",
        "        self.covariance_matrix = torch.nn.Parameter(torch.as_tensor(log_std))\n",
        "        self.mean_action_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            activation,\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            activation,\n",
        "        )\n",
        "\n",
        "    def _distribution(self, observation):\n",
        "        mean_act = self.mean_action_net(observation)\n",
        "        covariance_mat = torch.exp(self.covariance_matrix)\n",
        "        return Normal(mean_act, covariance_mat)\n",
        "\n",
        "    def _log_probs_from_dist(self, policy_dist, action):\n",
        "        return policy_dist.log_prob(action).sum(axis=-1)\n",
        "\n",
        "    def forward(self, observation, action=None):\n",
        "        policy_dist = self._distribution(observation)\n",
        "        logp_act = None\n",
        "        if action is not None:\n",
        "            logp_act = self._log_probs_from_dist(policy_dist, action)\n",
        "        return policy_dist, logp_act\n",
        "\n",
        "\n",
        "class ValueFunctionLearner(nn.Module):\n",
        "    def __init__(self, obs_dim: int, hidden_dim: int, activation) -> None:\n",
        "        super(ValueFunctionLearner, self).__init__()\n",
        "        self.v_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            activation,\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            activation,\n",
        "        )\n",
        "\n",
        "    def forward(self, observation):\n",
        "        return torch.squeeze(self.v_net(observation), -1)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        action_dim: int,\n",
        "        hidden_dim: int = 32,\n",
        "        activation=nn.Softmax(dim=-1),\n",
        "        save_path: Path = None\n",
        "    ) -> None:\n",
        "        super(Agent, self).__init__()\n",
        "        self.policy = DiagonalGaussian(obs_dim, hidden_dim, action_dim, activation)\n",
        "        self.value_func = ValueFunctionLearner(obs_dim, hidden_dim, activation)\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def step(self, obs: torch.Tensor) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        with torch.no_grad():\n",
        "            policy_dist = self.policy._distribution(obs)\n",
        "            action = policy_dist.sample()\n",
        "            action[0] = 1\n",
        "            mean_action = self.policy._log_probs_from_dist(policy_dist, action)\n",
        "            value = self.value_func(obs)\n",
        "        return action.numpy(), value.numpy(), mean_action.numpy()\n",
        "\n",
        "    def act(self, obs: torch.Tensor) -> np.ndarray:\n",
        "        return self.step(obs)[0]\n",
        "\n",
        "    def save_model(self) -> None:\n",
        "        assert self.save_path is not None\n",
        "        torch.save(self.policy.state_dict(), Path(self.save_path, \"policy\"))\n",
        "        torch.save(self.value_func.state_dict(), Path(self.save_path, \"value_function\"))\n",
        "\n",
        "    def load_model(self, eval: bool = False) -> None:\n",
        "        assert self.save_path is not None\n",
        "        pi = torch.load(Path(self.save_path, \"policy\"))\n",
        "        val = torch.load(Path(self.save_path, \"value_function\"))\n",
        "        self.policy.load_state_dict(pi)\n",
        "        self.value_func.load_state_dict(val)\n",
        "        if eval:\n",
        "            self.policy.eval()\n",
        "            self.value_func.eval()"
      ],
      "metadata": {
        "id": "DQZsxQSqUZCO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities\n",
        "* Retur Estimators\n",
        "* Advantage Function\n",
        "* Trajectory Buffer"
      ],
      "metadata": {
        "id": "SRZP_GwEki_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReturnEstimator(ABC):\n",
        "    @abstractmethod\n",
        "    def get_return(self, rewards: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculates a return over a trajectory\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "@dataclass\n",
        "class DiscountReturn(ReturnEstimator):\n",
        "    \"\"\"Calculate the discounted return over a trajectory, with discount factor gamma.\"\"\"\n",
        "    gamma: float = 0.99\n",
        "\n",
        "    def get_return(self, rewards: np.ndarray) -> np.ndarray:\n",
        "        pot = np.cumsum(np.ones(len(rewards))) - 1\n",
        "        g = np.full(len(pot), fill_value=self.gamma)\n",
        "        discount_gamma = g**pot\n",
        "        return rewards * discount_gamma\n",
        "\n",
        "class Advantage(ABC):\n",
        "    @abstractmethod\n",
        "    def estimate(self, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculates an advantage.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "@dataclass\n",
        "class GAE(Advantage):\n",
        "    return_estimator: ReturnEstimator\n",
        "    lamda: float = 0.5\n",
        "    gamma: float = 0.99\n",
        "\n",
        "    def estimate(self, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:\n",
        "        rew = np.append(rewards, 0)\n",
        "        val = np.append(values, 0)\n",
        "        deltas = rew[:-1] + (self.gamma * val[1:]) - val[:-1]\n",
        "        adv = self.return_estimator.get_return(deltas)\n",
        "        return adv\n",
        "\n",
        "class TrajectoryReplayBuffer:\n",
        "    \"\"\"A buffer for storing trajectory data\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        return_estimator: ReturnEstimator,\n",
        "        advantage: Advantage,\n",
        "        buf_size: int,\n",
        "        obs_dim: int,\n",
        "        act_dim: int,\n",
        "    ) -> None:\n",
        "        self._ret_estimator = return_estimator\n",
        "        self._adv_estimator = advantage\n",
        "        self._buf_size = buf_size\n",
        "        self._obs = np.zeros((buf_size, obs_dim), dtype=float)\n",
        "        self._act = np.zeros((buf_size, act_dim), dtype=float)\n",
        "        self._val = np.zeros(buf_size, dtype=float)\n",
        "        self._rewards = np.zeros(buf_size, dtype=float)\n",
        "        self._mean_act = np.zeros(buf_size, dtype=float)\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        idx: int,\n",
        "        obs: np.ndarray,\n",
        "        action: np.ndarray,\n",
        "        value: np.ndarray,\n",
        "        reward: float,\n",
        "        mean_act: float,\n",
        "    ) -> None:\n",
        "        assert idx < self._buf_size\n",
        "        self._obs[idx] = obs\n",
        "        self._act[idx] = action\n",
        "        self._val[idx] = value\n",
        "        self._rewards[idx] = reward\n",
        "        self._mean_act[idx] = mean_act\n",
        "\n",
        "    def finish_trajectory(self):\n",
        "        self._ret = self._ret_estimator.get_return(self._rewards)\n",
        "        self._adv = self._adv_estimator.estimate(self._rewards, self._val)\n",
        "\n",
        "    def get_trajectories(self):\n",
        "        data = dict(\n",
        "            obs=self._obs,\n",
        "            act=self._act,\n",
        "            val=self._val,\n",
        "            ret=self._ret,\n",
        "            logp=self._mean_act,\n",
        "            adv=self._adv,\n",
        "        )\n",
        "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in data.items()}"
      ],
      "metadata": {
        "id": "57fcN972U4Cy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss functions & Update function"
      ],
      "metadata": {
        "id": "q_6XjX8Hkyjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ppo_policy_loss(\n",
        "    actor_critic,\n",
        "    clip_ratio: float,\n",
        "    obs: torch.Tensor,\n",
        "    act: torch.Tensor,\n",
        "    adv: torch.Tensor,\n",
        "    logp_old: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    pi, logp = actor_critic.policy(obs, act)\n",
        "    ratio = torch.exp(logp - logp_old)\n",
        "    clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
        "    loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
        "    return loss_pi\n",
        "\n",
        "def ppo_value_loss(\n",
        "    actor_critic,\n",
        "    obs: torch.Tensor,\n",
        "    ret: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    return ((actor_critic.value_func(obs) - ret)**2).mean()\n",
        "\n",
        "def update(agent: Agent, data: dict, update_cycles: int) -> None:\n",
        "    for _ in range(update_cycles):\n",
        "        pi_optim.zero_grad()\n",
        "        pi_loss = ppo_policy_loss(agent,0.2,data[\"obs\"],data[\"act\"],data[\"adv\"],data[\"logp\"],)\n",
        "        pi_loss.backward()\n",
        "        pi_optim.step()\n",
        "\n",
        "        val_optim.zero_grad()\n",
        "        v_loss = ppo_value_loss(agent, data[\"obs\"], data[\"ret\"])\n",
        "        v_loss.backward()\n",
        "        val_optim.step()"
      ],
      "metadata": {
        "id": "lcwX0D-FHxBa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Space Description, Hyperparams & Main Loop"
      ],
      "metadata": {
        "id": "TmLUEJ5OlBA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Action Space\n",
        "    The action is a `ndarray` with shape `(1,)` representing the torque applied to free end of the pendulum.\n",
        "    | Num | Action | Min  | Max |\n",
        "    |-----|--------|------|-----|\n",
        "    | 0   | Torque | -2.0 | 2.0 |\n",
        "### Observation Space\n",
        "    The observation is a `ndarray` with shape `(3,)` representing the x-y coordinates of the pendulum's free end and its angular velocity.\n",
        "    | Num | Observation      | Min  | Max |\n",
        "    |-----|------------------|------|-----|\n",
        "    | 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
        "    | 1   | y = sin(angle)   | -1.0 | 1.0 |\n",
        "    | 2   | Angular Velocity | -8.0 | 8.0 |"
      ],
      "metadata": {
        "id": "TiW62qOyDh3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting up Hyperparameters ###\n",
        "EPISODES = 2\n",
        "EPISODE_LEN = 10\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "OBS_DIM = 3\n",
        "ACT_DIM = 1\n",
        "HIDDEN_DIM = 32\n",
        "BUF_SIZE = EPISODE_LEN\n",
        "\n",
        "GAMMA, LAMDA = 0.99, 0.5\n",
        "# --------------- #\n",
        "return_estimator = DiscountReturn(gamma=GAMMA)\n",
        "advantage_return = DiscountReturn(gamma=GAMMA * LAMDA)\n",
        "advantage = GAE(advantage_return, lamda=LAMDA, gamma=GAMMA)\n",
        "\n",
        "trajectory_buffer = TrajectoryReplayBuffer(\n",
        "    return_estimator,\n",
        "    advantage,\n",
        "    buf_size=BUF_SIZE,\n",
        "    obs_dim=OBS_DIM,\n",
        "    act_dim=ACT_DIM,\n",
        ")\n",
        "### Init Agent ###\n",
        "agent = Agent(\n",
        "    obs_dim=OBS_DIM,\n",
        "    action_dim=ACT_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    activation=nn.Softmax(dim=-1)\n",
        ")\n",
        "### Init Agent ###\n",
        "\n",
        "### Init Optimizer ###\n",
        "pi_optim = Adam(agent.policy.parameters(), lr=LEARNING_RATE)\n",
        "val_optim = Adam(agent.value_func.parameters(), lr=LEARNING_RATE)\n",
        "### Init Optimizer ###"
      ],
      "metadata": {
        "id": "mWR86ssXb-mW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training Loop ###\n",
        "env = gym.make(\"Pendulum-v0\")\n",
        "env = gym.wrappers.FlattenObservation(env)\n",
        "for _ in range(EPISODES): \n",
        "    obs = env.reset()\n",
        "    for t in range(EPISODE_LEN):\n",
        "      act, value, mean_act = agent.step(torch.as_tensor(obs, dtype=torch.float32))\n",
        "      # print(f\"action: {act}\\nvalue: {value}\\nmean action: {mean_act}\")\n",
        "      obs, reward, done, _ = env.step(act)\n",
        "      # print(f\"observation: {obs}\\nreward: {reward}\\ndone: {done}\")\n",
        "      # print(\"-------------------\")\n",
        "      trajectory_buffer.store(t, obs, act, value, reward, mean_act)\n",
        "      if done:\n",
        "          print(f\"Episode finished after {t} timesteps\")\n",
        "          break\n",
        "\n",
        "    trajectory_buffer.finish_trajectory()\n",
        "    # print(f\"advantage: {trajectory_buffer._adv},\n",
        "    # value targets(estimated return): {trajectory_buffer._ret}\")\n",
        "    data = trajectory_buffer.get_trajectories()\n",
        "\n",
        "    update(agent, data, 80)\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "MGF1G7B2cLTl"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}