{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s2bFrpvI2Wl"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQZsxQSqUZCO"
      },
      "outputs": [],
      "source": [
        "class DiagonalGaussian(nn.Module):\n",
        "    def __init__(\n",
        "        self, obs_dim: int, hidden_dim: int, action_dim: int, activation\n",
        "    ) -> None:\n",
        "        super(DiagonalGaussian, self).__init__()\n",
        "        log_std = -0.5 * np.ones(action_dim, dtype=float)\n",
        "        self.covariance_matrix = torch.nn.Parameter(torch.as_tensor(log_std))\n",
        "        self.mean_action_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            activation,\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            activation,\n",
        "        )\n",
        "\n",
        "    def _distribution(self, observation):\n",
        "        mean_act = self.mean_action_net(observation)\n",
        "        covariance_mat = torch.exp(self.covariance_matrix)\n",
        "        return Normal(mean_act, covariance_mat)\n",
        "\n",
        "    def _log_probs_from_dist(self, policy_dist, action):\n",
        "        return policy_dist.log_prob(action).sum(axis=-1)\n",
        "\n",
        "    def forward(self, observation, action=None):\n",
        "        policy_dist = self._distribution(observation)\n",
        "        logp_act = None\n",
        "        if action is not None:\n",
        "            logp_act = self._log_probs_from_dist(policy_dist, action)\n",
        "        return policy_dist, logp_act\n",
        "\n",
        "\n",
        "class ValueFunctionLearner(nn.Module):\n",
        "    def __init__(\n",
        "        self, obs_dim: int, hidden_dim: int, action_dim: int, activation\n",
        "    ) -> None:\n",
        "        super(ValueFunctionLearner, self).__init__()\n",
        "        self.v_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            activation,\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            activation,\n",
        "        )\n",
        "\n",
        "    def forward(self, observation):\n",
        "        # return torch.squeeze(self.v_net(observation), -1)\n",
        "        return self.v_net(observation)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        action_dim: int,\n",
        "        hidden_dim: int = 32,\n",
        "        activation=nn.Softmax(dim=-1),\n",
        "    ) -> None:\n",
        "        super(Agent, self).__init__()\n",
        "        self.policy = DiagonalGaussian(obs_dim, hidden_dim, action_dim, activation)\n",
        "        self.value_func = ValueFunctionLearner(\n",
        "            obs_dim, hidden_dim, action_dim, activation\n",
        "        )\n",
        "\n",
        "    def step(self, obs: torch.Tensor):\n",
        "        with torch.no_grad():\n",
        "            policy_dist = self.policy._distribution(obs)\n",
        "            action = policy_dist.sample()\n",
        "            action[0] = 1\n",
        "            mean_action = self.policy._log_probs_from_dist(policy_dist, action)\n",
        "            value = self.value_func(obs)\n",
        "        return action.numpy(), value.numpy(), mean_action.numpy()\n",
        "\n",
        "    def act(self, obs: torch.Tensor):\n",
        "        return self.step(obs)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57fcN972U4Cy"
      },
      "outputs": [],
      "source": [
        "class Advantage(ABC):\n",
        "    @abstractmethod\n",
        "    def estimate(self, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "class ReturnEstimator(ABC):\n",
        "    @abstractmethod\n",
        "    def get_return(self, rewards: np.ndarray, gamma: float) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "\n",
        "@dataclass\n",
        "class DiscountReturn(ReturnEstimator):\n",
        "    def get_return(self, rewards: np.ndarray, gamma: float = 0.99) -> np.ndarray:\n",
        "        pot = np.cumsum(np.ones(len(rewards))) - 1\n",
        "        g = np.full(len(pot), fill_value=gamma)\n",
        "        discount_gamma = g**pot\n",
        "        return rewards * discount_gamma\n",
        "        \n",
        "@dataclass\n",
        "class GAE(Advantage):\n",
        "    return_estimator: ReturnEstimator\n",
        "    lamda: Optional[float] = 0.5\n",
        "    gamma: Optional[float] = 0.99\n",
        "\n",
        "    def estimate(self, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:\n",
        "        rew = np.append(rewards, 0)\n",
        "        val = np.append(values, 0)\n",
        "        deltas = rew[:-1] + (self.gamma * val[1:]) - val[:-1]\n",
        "        adv = self.return_estimator.get_return(deltas, self.lamda*self.gamma)\n",
        "        return adv\n",
        "\n",
        "\n",
        "class TrajectoryReplayBuffer:\n",
        "    \"\"\"A buffer class for storing trajectory data\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        return_estimator: ReturnEstimator,\n",
        "        advantage: Advantage,\n",
        "        obs_dim: int,\n",
        "        act_dim: int,\n",
        "        val_dim: int,\n",
        "        buf_size: int,\n",
        "    ) -> None:\n",
        "        self._ret_estimator = return_estimator\n",
        "        self._adv_estimator = advantage\n",
        "        self._buf_size = buf_size\n",
        "        self._obs = np.zeros((buf_size, obs_dim), dtype=float)\n",
        "        self._act = np.zeros((buf_size, act_dim), dtype=float)\n",
        "        self._val = np.zeros((buf_size, val_dim), dtype=float)\n",
        "        self._rewards = np.zeros(buf_size, dtype=float)\n",
        "        self._mean_act = np.zeros(buf_size, dtype=float)\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        idx: int,\n",
        "        obs: np.ndarray,\n",
        "        action: np.ndarray,\n",
        "        value: np.ndarray,\n",
        "        reward: float,\n",
        "        mean_act: float,\n",
        "    ) -> None:\n",
        "        assert idx < self._buf_size\n",
        "        self._obs[idx] = obs\n",
        "        self._act[idx] = action\n",
        "        self._val[idx] = value\n",
        "        self._rewards[idx] = reward\n",
        "        self._mean_act[idx] = mean_act\n",
        "\n",
        "    def finish_trajectory(self):\n",
        "        self._adv = self._adv_estimator.estimate(self._rewards, self._val)\n",
        "        self._ret = self._ret_estimator.get_return(self._rewards, gamma=0.99)\n",
        "\n",
        "    def get_trajectories(self):\n",
        "        data = dict(\n",
        "            obs=self._obs,\n",
        "            act=self._act,\n",
        "            val=self._val,\n",
        "            ret=self._ret,\n",
        "            logp=self._mean_act,\n",
        "            adv=self._adv\n",
        "        )\n",
        "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in data.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiW62qOyDh3R"
      },
      "source": [
        "### Action Space\n",
        "    The action is a `ndarray` with shape `(1,)` representing the torque applied to free end of the pendulum.\n",
        "    | Num | Action | Min  | Max |\n",
        "    |-----|--------|------|-----|\n",
        "    | 0   | Torque | -2.0 | 2.0 |\n",
        "### Observation Space\n",
        "    The observation is a `ndarray` with shape `(3,)` representing the x-y coordinates of the pendulum's free end and its angular velocity.\n",
        "    | Num | Observation      | Min  | Max |\n",
        "    |-----|------------------|------|-----|\n",
        "    | 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
        "    | 1   | y = sin(angle)   | -1.0 | 1.0 |\n",
        "    | 2   | Angular Velocity | -8.0 | 8.0 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcwX0D-FHxBa"
      },
      "outputs": [],
      "source": [
        "def policy_loss(\n",
        "    actor_critic,\n",
        "    clip_ratio: float,\n",
        "    obs: torch.Tensor,\n",
        "    act: torch.Tensor,\n",
        "    adv: torch.Tensor,\n",
        "    logp_old: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    pi, logp = actor_critic.policy(obs, act)\n",
        "    ratio = torch.exp(logp - logp_old)\n",
        "    clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
        "    loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
        "    return loss_pi\n",
        "\n",
        "def value_loss(\n",
        "    actor_critic,\n",
        "    obs: torch.Tensor,\n",
        "    ret: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    return ((actor_critic.value_func(obs) - ret)**2).mean()\n",
        "\n",
        "def update(agent, data, update_cycles: int) -> None:\n",
        "\n",
        "    for _ in range(update_cycles):\n",
        "        pi_optim.zero_grad()\n",
        "        val_optim.zero_grad()\n",
        "\n",
        "        pi_loss = policy_loss(\n",
        "            agent, 0.2, data['obs'], data['act'], data['adv'], data['logp']\n",
        "        )\n",
        "        pi_loss.backward()\n",
        "\n",
        "        v_loss = value_loss(agent, data['obs'], data['ret'])\n",
        "        v_loss.backward()\n",
        "\n",
        "        pi_optim.step()\n",
        "        val_optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWR86ssXb-mW"
      },
      "outputs": [],
      "source": [
        "### Setting up Hyperparameters ###\n",
        "episodes = 1\n",
        "episode_len = 100\n",
        "learning_rate = 0.0001\n",
        "\n",
        "obs_dim = 3\n",
        "act_dim = 1\n",
        "val_dim = 1\n",
        "hidden_dim = 32\n",
        "buf_size = episode_len\n",
        "\n",
        "### Init Agent ###\n",
        "trajectory_buffer = TrajectoryReplayBuffer(\n",
        "    GAE,\n",
        "    DiscountReturn,\n",
        "    obs_dim=obs_dim,\n",
        "    act_dim=act_dim,\n",
        "    val_dim=val_dim,\n",
        "    buf_size=buf_size\n",
        ")\n",
        "agent = Agent(\n",
        "    obs_dim=obs_dim,\n",
        "    action_dim=act_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    activation=nn.Softmax(dim=-1)\n",
        ")\n",
        "### Init Agent ###\n",
        "\n",
        "### Init Optimizer ###\n",
        "pi_optim = Adam(agent.policy.parameters(), lr=learning_rate)\n",
        "val_optim = Adam(agent.value_func.parameters(), lr=learning_rate)\n",
        "### Init Optimizer ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGF1G7B2cLTl"
      },
      "outputs": [],
      "source": [
        "### Training Loop ###\n",
        "env = gym.make(\"Pendulum-v0\")\n",
        "env = gym.wrappers.FlattenObservation(env)\n",
        "for _ in range(episodes): \n",
        "    obs = env.reset()\n",
        "    for t in range(episode_len):\n",
        "      act, value, mean_act = agent.step(torch.as_tensor(obs, dtype=torch.float32))\n",
        "      # print(f\"action: {act}\\nvalue: {value}\\nmean action: {mean_act}\")\n",
        "      obs, reward, done, _ = env.step(act)\n",
        "      # print(f\"observation: {obs}\\nreward: {reward}\\ndone: {done}\")\n",
        "      # print(\"-------------------\")\n",
        "      trajectory_buffer.store(t, obs, act, value, reward, mean_act)\n",
        "      if done:\n",
        "          print(f\"Episode finished after {t} timesteps\")\n",
        "          break\n",
        "\n",
        "    trajectory_buffer.finish_trajectory()\n",
        "    # print(f\"advantage: {trajectory_buffer._adv},\n",
        "    # value targets(estimated return): {trajectory_buffer._ret}\")\n",
        "    data = trajectory_buffer.get_trajectories()\n",
        "\n",
        "    update(agent, data, 80)\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ContinousControl_ParallelTestEnv.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
