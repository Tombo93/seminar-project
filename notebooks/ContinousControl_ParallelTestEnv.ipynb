{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ContinousControl_ParallelTestEnv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8s2bFrpvI2Wl"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DiagonalGaussian(nn.Module):\n",
        "    def __init__(\n",
        "        self, obs_dim: int, hidden_dim: int, action_dim: int, activation\n",
        "    ) -> None:\n",
        "        super(DiagonalGaussian, self).__init__()\n",
        "        log_std = -0.5 * np.ones(action_dim, dtype=float)\n",
        "        self.covariance_matrix = torch.nn.Parameter(torch.as_tensor(log_std))\n",
        "        self.mean_action_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            activation,\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            activation,\n",
        "        )\n",
        "\n",
        "    def _distribution(self, observation):\n",
        "        mean_act = self.mean_action_net(observation)\n",
        "        covariance_mat = torch.exp(self.covariance_matrix)\n",
        "        return Normal(mean_act, covariance_mat)\n",
        "\n",
        "    def _log_probs_from_dist(self, policy_dist, action):\n",
        "        return policy_dist.log_prob(action).sum(axis=-1)\n",
        "\n",
        "    def forward(self, observation, action=None):\n",
        "        policy_dist = self._distribution(observation)\n",
        "        logp_act = None\n",
        "        if action is not None:\n",
        "            logp_act = self._log_probs_from_dist(policy_dist, action)\n",
        "        return policy_dist, logp_act\n",
        "\n",
        "\n",
        "class ValueFunctionLearner(nn.Module):\n",
        "    def __init__(\n",
        "        self, obs_dim: int, hidden_dim: int, action_dim: int, activation\n",
        "    ) -> None:\n",
        "        super(ValueFunctionLearner, self).__init__()\n",
        "        self.v_net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_dim),\n",
        "            activation,\n",
        "            nn.Linear(hidden_dim, action_dim),\n",
        "            activation,\n",
        "        )\n",
        "\n",
        "    def forward(self, observation):\n",
        "        # return torch.squeeze(self.v_net(observation), -1)\n",
        "        return self.v_net(observation)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        action_dim: int,\n",
        "        hidden_dim: int = 32,\n",
        "        activation=nn.Softmax(dim=-1),\n",
        "    ) -> None:\n",
        "        super(Agent, self).__init__()\n",
        "        self.policy = DiagonalGaussian(obs_dim, hidden_dim, action_dim, activation)\n",
        "        self.value_func = ValueFunctionLearner(\n",
        "            obs_dim, hidden_dim, action_dim, activation\n",
        "        )\n",
        "\n",
        "    def step(self, obs: torch.Tensor):\n",
        "        with torch.no_grad():\n",
        "            policy_dist = self.policy._distribution(obs)\n",
        "            action = policy_dist.sample()\n",
        "            action[0] = 1\n",
        "            mean_action = self.policy._log_probs_from_dist(policy_dist, action)\n",
        "            value = self.value_func(obs)\n",
        "        return action.numpy(), value.numpy(), mean_action.numpy()\n",
        "\n",
        "    def act(self, obs: torch.Tensor):\n",
        "        return self.step(obs)[0]\n"
      ],
      "metadata": {
        "id": "DQZsxQSqUZCO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Advantage(ABC):\n",
        "    @abstractmethod\n",
        "    def estimate(self, values: np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "class ReturnEstimator(ABC):\n",
        "    @abstractmethod\n",
        "    def get_return(self, rewards: np.ndarray, gamma: float) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "\n",
        "@dataclass\n",
        "class DiscountReturn(ReturnEstimator):\n",
        "    def get_return(self, rewards: np.ndarray, gamma: float = 0.99) -> np.ndarray:\n",
        "        pot = np.cumsum(np.ones(len(rewards))) - 1\n",
        "        g = np.full(len(pot), fill_value=gamma)\n",
        "        discount_gamma = g**pot\n",
        "        return rewards * discount_gamma\n",
        "        \n",
        "@dataclass\n",
        "class GAE(Advantage):\n",
        "    return_estimator: ReturnEstimator\n",
        "    lamda: Optional[float] = 0.5\n",
        "    gamma: Optional[float] = 0.99\n",
        "\n",
        "    def estimate(self, rewards: np.ndarray, values: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        rew = np.append(rewards, 0)\n",
        "        val = np.append(values, 0)\n",
        "        deltas = rew[:-1] + (self.gamma * val[1:]) - val[:-1]\n",
        "        adv = self.return_estimator.get_return(deltas, self.lamda*self.gamma)\n",
        "        ret = self.return_estimator.get_return(rew, self.gamma)[:-1] # value function targets\n",
        "        return adv, ret\n",
        "\n",
        "\n",
        "class TrajectoryReplayBuffer:\n",
        "    \"\"\"A buffer class for storing trajectory data\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        advantage: Advantage,\n",
        "        return_estimator: ReturnEstimator,\n",
        "        obs_dim: int,\n",
        "        act_dim: int,\n",
        "        val_dim: int,\n",
        "        buf_size: int,\n",
        "    ) -> None:\n",
        "        self._buf_size = buf_size\n",
        "        self._ret_estimator = return_estimator()\n",
        "        self._adv_estimator = advantage(self._ret_estimator)\n",
        "        self._obs = np.zeros((buf_size, obs_dim), dtype=float)\n",
        "        self._act = np.zeros((buf_size, act_dim), dtype=float)\n",
        "        self._val = np.zeros((buf_size, val_dim), dtype=float)\n",
        "        self._adv = np.zeros(buf_size, dtype=float)\n",
        "        self._mean_act = np.zeros(buf_size, dtype=float)\n",
        "        self._rewards = np.zeros(buf_size, dtype=float)\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        idx: int,\n",
        "        action: np.ndarray,\n",
        "        value: np.ndarray,\n",
        "        reward: float,\n",
        "        mean_act: float,\n",
        "    ) -> None:\n",
        "        assert idx < self._buf_size\n",
        "        self._act[idx] = action\n",
        "        self._val[idx] = value\n",
        "        self._rewards[idx] = reward\n",
        "        self._mean_act[idx] = mean_act\n",
        "\n",
        "    def compute_advantage(self):\n",
        "        self._adv, value_targets = self._adv_estimator.estimate(self._rewards, self._val)\n",
        "        return self._adv, value_targets\n",
        "\n",
        "    def get_trajectories(self):\n",
        "        data = dict(V=self._val)\n",
        "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in data.items()}\n",
        "\n",
        "    def expected_returns(self, arr: np.ndarray) -> np.ndarray:\n",
        "        expected_returns = np.zeros(arr.shape)\n",
        "        for i in reversed(range(len(arr))):\n",
        "            ret_t = self._ret_estimator(arr[i:])\n",
        "            expected_returns[i] = ret_t\n",
        "        return expected_returns"
      ],
      "metadata": {
        "id": "57fcN972U4Cy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Setting up Hyperparameters ###\n",
        "\n",
        "episodes = 1\n",
        "episode_len = 1\n",
        "\n",
        "obs_dim = 6\n",
        "act_dim = 6\n",
        "val_dim = 6\n",
        "hidden_dim = 32\n",
        "buf_size = episode_len\n",
        "\n",
        "### Init Agent ###\n",
        "trajectory_buffer = TrajectoryReplayBuffer(\n",
        "    GAE,\n",
        "    DiscountReturn,\n",
        "    obs_dim=obs_dim,\n",
        "    act_dim=act_dim,\n",
        "    val_dim=val_dim,\n",
        "    buf_size=buf_size\n",
        ")\n",
        "agent = Agent(\n",
        "    obs_dim=obs_dim,\n",
        "    action_dim=act_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    activation=nn.Softmax(dim=-1)\n",
        ")\n",
        "### Init Agent ###"
      ],
      "metadata": {
        "id": "mWR86ssXb-mW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training Loop ###\n",
        "env = gym.make(\"FetchReach-v1\")\n",
        "env = gym.wrappers.FlattenObservation(env)\n",
        "for _ in range(episodes): \n",
        "  obs = env.reset()\n",
        "  for t in range(episode_len):\n",
        "    env.render()\n",
        "    # action = env.action_space.sample()\n",
        "    a = agent.act(obs)\n",
        "    obs, reward, done, info = env.step(a)\n",
        "    \n",
        "    if done:\n",
        "      print(f\"Episode finished after {t} timesteps\")\n",
        "      break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "MGF1G7B2cLTl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}